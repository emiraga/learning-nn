# learning-nn
Learning Neural Networks from various sources and I will document and track my progress here

## Lesson 1. Neural Networks: Zero to Hero -  Andrej Karpathy

* https://github.com/karpathy/nn-zero-to-hero
* https://karpathy.ai/zero-to-hero.html

## Lesson 2. GPT in 60 Lines of NumPy
* https://jaykmody.com/blog/gpt-from-scratch/#setup
* https://news.ycombinator.com/item?id=34726115


# Advice from GTP-4

> what should be my learning path, to be able to eventually understand how to make GPT or to understand how they work?

To gain a deep understanding of how GPT models work and eventually be able to create one, you will need to build a solid foundation in various areas of computer science, mathematics, and machine learning. Here is a suggested learning path:

1. Programming: Learn a programming language, such as Python, which is widely used in the AI and machine learning community. Familiarize yourself with relevant libraries and frameworks like NumPy, Pandas, and TensorFlow or PyTorch.

2. Mathematics: Gain a strong understanding of linear algebra, calculus, probability, and statistics, as these subjects are essential for understanding machine learning algorithms and concepts.

3. Data Science: Learn the basics of data science, including data manipulation, visualization, and analysis. This will help you work with large datasets and prepare data for machine learning models.

4. Machine Learning: Study the fundamentals of machine learning, including supervised and unsupervised learning, various algorithms (such as linear regression, decision trees, and neural networks), and techniques for model evaluation and selection.

5. Deep Learning: Dive into deep learning, focusing on neural networks and their architectures, including convolutional neural networks (CNNs), recurrent neural networks (RNNs), and long short-term memory (LSTM) networks. Learn about backpropagation, optimization techniques, and regularization methods.

6. Natural Language Processing (NLP): Understand the basics of NLP, including tokenization, stemming, lemmatization, part-of-speech tagging, named entity recognition, and sentiment analysis. Study advanced NLP techniques like word embeddings, attention mechanisms, and sequence-to-sequence models.

7. Transformer Architecture: Learn about the Transformer architecture, which is the basis for GPT models. Understand concepts like self-attention, positional encoding, and the overall architecture, including the encoder-decoder structure.

8. Pre-training and Fine-tuning: Study the process of pre-training and fine-tuning large language models, including unsupervised learning, transfer learning, and the use of techniques like masked language modeling and next sentence prediction.

9. GPT Models: Deepen your understanding of GPT models by studying their architecture, training, and use in various NLP tasks. Review research papers and articles on GPT-2, GPT-3, and subsequent versions, as well as their limitations and ethical considerations.

10. Hands-on Experience: Work on projects and experiments to apply your knowledge and build practical skills. Use available pre-trained GPT models, fine-tune them for specific tasks, and explore the latest developments in the field.

As you progress through this learning path, you will develop the knowledge and skills needed to understand GPT models and potentially create your own. Keep in mind that this is a complex field, and mastery will require dedication, practice, and continuous learning.